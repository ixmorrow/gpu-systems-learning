{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "lpWSl-IbjEr8"
      },
      "outputs": [],
      "source": [
        "# Build a Transformer architecture from scratch using only PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model    # embedding dimension (e.g., 512)\n",
        "    self.num_heads = num_heads  # number of attention heads (e.g., 8)\n",
        "    self.head_dim = d_model // num_heads  # dimension per head (e.g., 64)\n",
        "\n",
        "    # Create the Q, K, V projection layers\n",
        "    self.q_proj = nn.Linear(d_model, d_model)\n",
        "    self.k_proj = nn.Linear(d_model, d_model)\n",
        "    self.v_proj = nn.Linear(d_model, d_model)\n",
        "\n",
        "    # Final output projection\n",
        "    self.out_proj = nn.Linear(d_model, d_model)\n",
        "\n",
        "  # scaled dot product attention\n",
        "  def attention(self, Q, K, V, mask=None):\n",
        "    \"\"\"\n",
        "    Q, K, V are expected to be of shape:\n",
        "      [batch_size, seq_len, d_k]\n",
        "    or possibly\n",
        "      [batch_size, num_heads, seq_len, d_k]\n",
        "    if youâ€™re already doing multi-head splitting.\n",
        "    \"\"\"\n",
        "    d_k = K.shape[-1]\n",
        "    scores = Q @ K.transpose(-2, -1)\n",
        "    scores = scores / math.sqrt(d_k)\n",
        "    # Apply mask if provided\n",
        "    if mask is not None:\n",
        "        # Expand mask to match the batch size and number of heads\n",
        "        # mask shape: [batch_size, 1, seq_len, seq_len] or [batch_size, 1, 1, seq_len]\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "    attention_weights = F.softmax(scores, dim=-1)\n",
        "    output = attention_weights @ V\n",
        "\n",
        "    return output, attention_weights\n",
        "\n",
        "  def transopse_akv(self, output, attention_weights, batch_size, seq_len, d_model):\n",
        "    # re-order dimensions back to original\n",
        "    output = torch.permute(output, (0, 2, 1, 3))\n",
        "    # reshape the dimensions to \"combine\" the attention heads outputs\n",
        "    output = output.reshape(batch_size, seq_len, d_model)\n",
        "    # attention_weights has shape [batch_size, num_heads, seq_len, seq_len]\n",
        "    # Average across the heads dimension (dim=1)\n",
        "    attention_weights = attention_weights.mean(dim=1)\n",
        "\n",
        "    return output, attention_weights\n",
        "\n",
        "  def forward(self, q, k, v, mask=None):\n",
        "    batch_size, seq_len, d_model = q.shape\n",
        "    Q = self.q_proj(q)\n",
        "    K = self.k_proj(k)\n",
        "    V = self.v_proj(v)\n",
        "\n",
        "    # Reshape to separate the heads\n",
        "    Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "    K = K.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "    V = V.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "\n",
        "    # re-order dimensions to be compatible with attention method\n",
        "    Q = torch.permute(Q, (0, 2, 1, 3))\n",
        "    K = torch.permute(K, (0, 2, 1, 3))\n",
        "    V = torch.permute(V, (0, 2, 1, 3))\n",
        "\n",
        "    output, attention_weights = self.attention(Q, K, V, mask)\n",
        "    output, attention_weights = self.transopse_akv(output, attention_weights, batch_size, seq_len, d_model)\n",
        "\n",
        "    return self.out_proj(output)"
      ],
      "metadata": {
        "id": "Jg5fYcoMkS-C"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        # Multi-head attention\n",
        "        self.self_attention = MultiHeadAttention(d_model, n_heads)\n",
        "\n",
        "        # Feed forward\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_ff, d_model)\n",
        "        )\n",
        "\n",
        "        # Normalization layers\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # Self attention\n",
        "        attn_output = self.self_attention(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output)) # residual connection\n",
        "\n",
        "        # Feed forward\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm2(x + self.dropout(ff_output)) # residual connection\n",
        "\n",
        "        return x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, d_model, n_heads, d_ff, num_layers, dropout=0.1):\n",
        "      super(Encoder, self).__init__()\n",
        "\n",
        "      # Stack of encoder layers\n",
        "      self.layers = nn.ModuleList([\n",
        "          EncoderLayer(d_model, n_heads, d_ff, dropout)\n",
        "          for _ in range(num_layers)\n",
        "      ])\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "      for layer in self.layers:\n",
        "          x = layer(x, mask)\n",
        "      return x"
      ],
      "metadata": {
        "id": "rYCvc5Lgn9la"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "  def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "    # TODO: Initialize three components:\n",
        "    # 1. Masked multi-head self-attention (for target sequence)\n",
        "    self.self_attention = MultiHeadAttention(d_model, n_heads)\n",
        "\n",
        "    # 2. Multi-head cross-attention (to attend to encoder outputs)\n",
        "    self.cross_attention = MultiHeadAttention(d_model, n_heads)\n",
        "\n",
        "    # 3. Feed-forward network\n",
        "    self.feed_forward = nn.Sequential(\n",
        "        nn.Linear(d_model, d_ff),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(d_ff, d_model)\n",
        "    )\n",
        "    # 4. Initialize normalization layers\n",
        "    self.norm1 = nn.LayerNorm(d_model)\n",
        "    self.norm2 = nn.LayerNorm(d_model)\n",
        "    self.norm3 = nn.LayerNorm(d_model)  # One more for the cross-attention\n",
        "\n",
        "    # 5. Initialize dropout\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x, enc_output, src_mask, tgt_mask):\n",
        "    # First sub-layer - masked self-attention on target sequence\n",
        "    # Remember to apply residual connection and layer normalization\n",
        "    attn_output = self.self_attention(x, x, x, tgt_mask)\n",
        "    x = self.norm1(x + self.dropout(attn_output))\n",
        "\n",
        "    # Second sub-layer - cross-attention with encoder outputs\n",
        "    # Remember to apply residual connection and layer normalization\n",
        "    cross_attn_output = self.cross_attention(x, enc_output, enc_output, src_mask)\n",
        "    x = self.norm2(x + self.dropout(cross_attn_output)) # residual connection\n",
        "\n",
        "    # Third sub-layer - feed-forward network\n",
        "    # Remember to apply residual connection and layer normalization\n",
        "    ff_output = self.feed_forward(x)\n",
        "    x = self.norm3(x + self.dropout(ff_output)) # this is the residual connection\n",
        "\n",
        "    return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, d_model, n_heads, d_ff, num_layers, dropout=0.1):\n",
        "    super(Decoder, self)._init__()\n",
        "\n",
        "    # Stack of decoder layers\n",
        "    self.layers = nn.ModuleList([\n",
        "        DecoderLayer(d_model, n_heads, d_ff, dropout)\n",
        "        for _ in range(num_layers)\n",
        "    ])\n",
        "\n",
        "  def forward(self, x, enc_output, src_mask, tgt_mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, enc_output, src_mask, tgt_mask)\n",
        "        return x"
      ],
      "metadata": {
        "id": "lehlodr-qDgG"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self,\n",
        "              src_vocab_size,\n",
        "              tgt_vocab_size,\n",
        "              d_model, # embedding dimension\n",
        "              num_heads,\n",
        "              num_encoder_layers,\n",
        "              num_decoder_layers,\n",
        "              d_ff,\n",
        "              max_seq_length,\n",
        "              dropout=0.1):\n",
        "      super(Transformer, self).__init__()\n",
        "      self.d_model = d_model\n",
        "      self.num_heads = num_heads\n",
        "      # Create embedding layers\n",
        "      self.src_embedding = nn.Embedding(src_vocab_size, d_model) # Question: Are source embeddings and target embeddings required here?\n",
        "      self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
        "      # Add positional encoding\n",
        "      self.positional_encoding = self.create_positional_encoding(max_seq_length, d_model)\n",
        "      self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "      # Create encoder\n",
        "      self.encoder = Encoder(d_model, num_heads, d_ff, num_encoder_layers, dropout)\n",
        "\n",
        "      # Create decoder\n",
        "      self.decoder = Decoder(d_model, num_heads, d_ff, num_decoder_layers, dropout)\n",
        "\n",
        "      # Create final output layer\n",
        "      self.output_layer = nn.Linear(d_model, tgt_vocab_size)\n",
        "\n",
        "  def create_positional_encoding(self, max_seq_length, d_model):\n",
        "    pe = torch.zeros(max_seq_length, d_model)\n",
        "    position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
        "    div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "    pe[:, 0::2] = torch.sin(position * div_term)\n",
        "    pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "    pe = pe.unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "    # Register as buffer (not a parameter, but should be saved and moved with the model)\n",
        "    return nn.Parameter(pe, requires_grad=False)\n",
        "\n",
        "  def forward(self, src, tgt, src_mask, tgt_mask):\n",
        "    # Get sequence lengths for positional encoding\n",
        "    src_seq_len = src.size(1)\n",
        "    tgt_seq_len = tgt.size(1)\n",
        "\n",
        "    # Embed source and add positional encoding\n",
        "    src_embedded = self.dropout(self.src_embedding(src) * math.sqrt(self.d_model) + self.positional_encoding[:, :src_seq_len, :])\n",
        "\n",
        "    # Pass through encoder\n",
        "    enc_output = self.encoder(src_embedded, src_mask)\n",
        "\n",
        "    # Embed target and add positional encoding\n",
        "    tgt_embedded = self.dropout(self.tgt_embedding(tgt) * math.sqrt(self.d_model) + self.positional_encoding[:, :tgt_seq_len, :])\n",
        "\n",
        "    # Pass through decoder (with encoder output)\n",
        "    dec_output = self.decoder(tgt_embedded, enc_output, src_mask, tgt_mask)\n",
        "\n",
        "    # Pass through final output layer and return raw logits\n",
        "    logits = self.output_layer(dec_output)\n",
        "\n",
        "    return logits"
      ],
      "metadata": {
        "id": "URvLredijmfq"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iOVfPP1n1xS8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}