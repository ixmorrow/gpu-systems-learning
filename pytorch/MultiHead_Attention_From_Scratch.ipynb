{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "q-kvLAU4bk4f"
      },
      "outputs": [],
      "source": [
        "# Build an Attention Neural Network using PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "xXKWE3ixbwqS"
      },
      "outputs": [],
      "source": [
        "class AttentionNeuralNet(nn.Module):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model    # embedding dimension (e.g., 512)\n",
        "    self.num_heads = num_heads  # number of attention heads (e.g., 8)\n",
        "    self.head_dim = d_model // num_heads  # dimension per head (e.g., 64)\n",
        "\n",
        "    # Create the Q, K, V projection layers\n",
        "    self.q_proj = nn.Linear(d_model, d_model)\n",
        "    self.k_proj = nn.Linear(d_model, d_model)\n",
        "    self.v_proj = nn.Linear(d_model, d_model)\n",
        "\n",
        "    # Final output projection\n",
        "    self.out_proj = nn.Linear(d_model, d_model)\n",
        "\n",
        "  # scaled dot product attention\n",
        "  def attention(self, Q, K, V):\n",
        "    \"\"\"\n",
        "    Q, K, V are expected to be of shape:\n",
        "      [batch_size, seq_len, d_k]\n",
        "    or possibly\n",
        "      [batch_size, num_heads, seq_len, d_k]\n",
        "    if you’re already doing multi-head splitting.\n",
        "    \"\"\"\n",
        "    d_k = K.shape[-1]\n",
        "    scores = Q @ K.transpose(-2, -1)\n",
        "    scores = scores / math.sqrt(d_k)\n",
        "    attention_weights = F.softmax(scores, dim=-1)\n",
        "    output = attention_weights @ V\n",
        "\n",
        "    return output, attention_weights\n",
        "\n",
        "  def transopse_akv(self, output, attention_weights, batch_size, seq_len, d_model):\n",
        "    # re-order dimensions back to original\n",
        "    output = torch.permute(output, (0, 2, 1, 3))\n",
        "    # reshape the dimensions to \"combine\" the attention heads outputs\n",
        "    output = output.reshape(batch_size, seq_len, d_model)\n",
        "    # attention_weights has shape [batch_size, num_heads, seq_len, seq_len]\n",
        "    # Average across the heads dimension (dim=1)\n",
        "    attention_weights = attention_weights.mean(dim=1)\n",
        "\n",
        "    return output, attention_weights\n",
        "\n",
        "  def forward(self, x):\n",
        "    batch_size, seq_len, d_model = x.shape\n",
        "    Q = self.q_proj(x)\n",
        "    K = self.k_proj(x)\n",
        "    V = self.v_proj(x)\n",
        "\n",
        "    # Reshape to separate the heads\n",
        "    Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "    K = K.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "    V = V.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "\n",
        "    # re-order dimensions to be compatible with attention method\n",
        "    Q = torch.permute(Q, (0, 2, 1, 3))\n",
        "    K = torch.permute(K, (0, 2, 1, 3))\n",
        "    V = torch.permute(V, (0, 2, 1, 3))\n",
        "\n",
        "    output, attention_weights = self.attention(Q, K, V)\n",
        "    output, attention_weights = self.transopse_akv(output, attention_weights, batch_size, seq_len, d_model)\n",
        "\n",
        "    output = self.out_proj(output)\n",
        "\n",
        "    return output, attention_weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "nkFUUch9xZI8"
      },
      "outputs": [],
      "source": [
        "def test_attention_shapes(batch_size=32, seq_len=10, d_model=512, num_heads=8):\n",
        "    # Create model\n",
        "    model = AttentionNeuralNet(d_model=d_model, num_heads=num_heads)\n",
        "\n",
        "    # Create dummy input\n",
        "    x = torch.randn(batch_size, seq_len, d_model)\n",
        "\n",
        "    # Forward pass\n",
        "    output, attention_weights = model(x)\n",
        "\n",
        "    # Check shapes\n",
        "    assert output.shape == (batch_size, seq_len, d_model)\n",
        "    assert attention_weights.shape == (batch_size, seq_len, seq_len)\n",
        "\n",
        "def test_attention_weights_sum_to_one(batch_size=32, seq_len=10, d_model=512, num_heads=8):\n",
        "  # Create model\n",
        "  model = AttentionNeuralNet(d_model=d_model, num_heads=num_heads)\n",
        "\n",
        "  # Create dummy input\n",
        "  x = torch.randn(batch_size, seq_len, d_model)\n",
        "\n",
        "  # Forward pass\n",
        "  output, attention_weights = model(x)\n",
        "  assert torch.allclose(attention_weights.sum(dim=-1), torch.ones_like(attention_weights.sum(dim=-1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "pKloLio3xnMm"
      },
      "outputs": [],
      "source": [
        "test_attention_shapes()\n",
        "test_attention_weights_sum_to_one()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "pbffLYzk1Bn3"
      },
      "outputs": [],
      "source": [
        "def create_sequence_dataset(num_sequences=1000, seq_length=10, d_model=512):\n",
        "    # Create random input sequences\n",
        "    X = torch.randn(num_sequences, seq_length, d_model)\n",
        "    # Create target sequences (initially same as input)\n",
        "    y = X.clone()\n",
        "\n",
        "    # For positions 2, 5, 8, etc., make the target the sum of previous two tokens\n",
        "    for pos in range(2, seq_length, 3):\n",
        "        y[:, pos] = X[:, pos-1] + X[:, pos-2]\n",
        "\n",
        "    return X, y\n",
        "\n",
        "# Let's test the dataset creation\n",
        "def test_dataset():\n",
        "    X, y = create_sequence_dataset(num_sequences=5, seq_length=10, d_model=4)\n",
        "    print(\"Input shape:\", X.shape)\n",
        "    print(\"Target shape:\", y.shape)\n",
        "\n",
        "    # Verify the pattern for first sequence\n",
        "    print(\"\\nFirst sequence, first few dimensions:\")\n",
        "    print(\"Position 2 should equal sum of positions 0 and 1:\")\n",
        "    print(f\"X[0, 0]: {X[0, 0][:2]}\")  # First token\n",
        "    print(f\"X[0, 1]: {X[0, 1][:2]}\")  # Second token\n",
        "    print(f\"y[0, 2]: {y[0, 2][:2]}\")  # Third token (should be sum)\n",
        "\n",
        "# Training loop\n",
        "def train_attention_model(model, num_epochs=10):\n",
        "    X_train, y_train = create_sequence_dataset()\n",
        "    optimizer = torch.optim.Adam(model.parameters())\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        optimizer.zero_grad()\n",
        "        output, _ = model(X_train)\n",
        "        loss = criterion(output, y_train)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if epoch % 2 == 0:\n",
        "            print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3fUb-qg1HRJ",
        "outputId": "1073157c-07bb-4c7a-a9d3-9b9d5a0261a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Loss: 1.3141\n",
            "Epoch 2, Loss: 1.2966\n",
            "Epoch 4, Loss: 1.2799\n",
            "Epoch 6, Loss: 1.2622\n",
            "Epoch 8, Loss: 1.2419\n",
            "Epoch 10, Loss: 1.2172\n",
            "Epoch 12, Loss: 1.1863\n",
            "Epoch 14, Loss: 1.1490\n",
            "Epoch 16, Loss: 1.1069\n",
            "Epoch 18, Loss: 1.0641\n",
            "Epoch 20, Loss: 1.0253\n",
            "Epoch 22, Loss: 0.9915\n",
            "Epoch 24, Loss: 0.9599\n",
            "Epoch 26, Loss: 0.9270\n",
            "Epoch 28, Loss: 0.8917\n",
            "Epoch 30, Loss: 0.8549\n",
            "Epoch 32, Loss: 0.8181\n",
            "Epoch 34, Loss: 0.7823\n",
            "Epoch 36, Loss: 0.7476\n",
            "Epoch 38, Loss: 0.7134\n",
            "Epoch 40, Loss: 0.6789\n",
            "Epoch 42, Loss: 0.6442\n",
            "Epoch 44, Loss: 0.6099\n",
            "Epoch 46, Loss: 0.5767\n",
            "Epoch 48, Loss: 0.5448\n"
          ]
        }
      ],
      "source": [
        "batch_size=32\n",
        "seq_len=10\n",
        "d_model=512\n",
        "num_heads=8\n",
        "model = AttentionNeuralNet(d_model=d_model, num_heads=num_heads)\n",
        "train_attention_model(model, num_epochs=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "pWw69ys82iRU"
      },
      "outputs": [],
      "source": [
        "def analyze_model(model, seq_length=10, d_model=512):\n",
        "    # Create a test sequence\n",
        "    X_test = torch.randn(1, seq_length, d_model)\n",
        "    y_test = X_test.clone()\n",
        "    for pos in range(2, seq_length, 3):\n",
        "        y_test[:, pos] = X_test[:, pos-1] + X_test[:, pos-2]\n",
        "\n",
        "    # Get model predictions and attention weights\n",
        "    with torch.no_grad():\n",
        "        pred, attention_weights = model(X_test)\n",
        "\n",
        "    # Calculate prediction error\n",
        "    mse = nn.MSELoss()(pred, y_test)\n",
        "    print(f\"Test MSE: {mse.item():.4f}\")\n",
        "\n",
        "    # Analyze attention patterns\n",
        "    print(\"\\nAttention patterns for summed positions:\")\n",
        "    for pos in range(2, seq_length, 3):\n",
        "        print(f\"\\nPosition {pos} attention weights:\")\n",
        "        print(attention_weights[0, pos, pos-2:pos+1])  # Show attention to previous tokens\n",
        "\n",
        "def analyze_predictions(model, seq_length=10, d_model=512):\n",
        "    X_test = torch.randn(1, seq_length, d_model)\n",
        "    y_test = X_test.clone()\n",
        "\n",
        "    # Create expected sums\n",
        "    for pos in range(2, seq_length, 3):\n",
        "        y_test[:, pos] = X_test[:, pos-1] + X_test[:, pos-2]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        pred, _ = model(X_test)\n",
        "\n",
        "    # Compare predictions with expected sums\n",
        "    for pos in range(2, seq_length, 3):\n",
        "        expected_sum = X_test[0, pos-2] + X_test[0, pos-1]\n",
        "        print(f\"\\nPosition {pos}:\")\n",
        "        print(f\"Expected sum: {expected_sum[:5]}\")  # Show first 5 dimensions\n",
        "        print(f\"Prediction:   {pred[0, pos][:5]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xm2H_tQS2j9P",
        "outputId": "99f490a2-56b3-417d-8d96-bb35e5bb6dbf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test MSE: 0.9548\n",
            "\n",
            "Attention patterns for summed positions:\n",
            "\n",
            "Position 2 attention weights:\n",
            "tensor([0.0551, 0.0303, 0.6893])\n",
            "\n",
            "Position 5 attention weights:\n",
            "tensor([0.0646, 0.0256, 0.6734])\n",
            "\n",
            "Position 8 attention weights:\n",
            "tensor([0.0313, 0.0477, 0.5942])\n"
          ]
        }
      ],
      "source": [
        "analyze_model(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xOerh2m3oQO",
        "outputId": "0b410320-8134-4047-fed0-10910dbf16b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Position 2:\n",
            "Expected sum: tensor([-3.8203, -0.2196, -0.1141, -0.7116,  0.2415])\n",
            "Prediction:   tensor([-0.2672, -0.2304,  0.3185,  0.7512,  1.3425])\n",
            "\n",
            "Position 5:\n",
            "Expected sum: tensor([ 0.1566, -0.0233,  0.6566, -1.4544, -2.7014])\n",
            "Prediction:   tensor([-1.0759, -1.0394,  0.6194, -1.0045,  0.4681])\n",
            "\n",
            "Position 8:\n",
            "Expected sum: tensor([ 0.1154, -0.6889,  1.8166, -0.6379, -1.6967])\n",
            "Prediction:   tensor([-0.3267, -0.0108, -0.0664, -0.3468, -0.2812])\n"
          ]
        }
      ],
      "source": [
        "analyze_predictions(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWu-G96lid31"
      },
      "source": [
        "# NOTES\n",
        "\n",
        "Input embedding dimension is the embedding of each token's embedding matrix that comes into the attention layer. -> Split words into tokens and each token is converted to an embedding matrix that numerically represents what the word is in some language embedding space.\n",
        "\n",
        "Attention weights have a different dimension than the outputs.\n",
        "\n",
        "For the attention weights:\n",
        "\n",
        "They come from the scores calculation: scores = Q @ K.transpose(-2, -1)\n",
        "\n",
        "* Q shape: [batch_size, num_heads, seq_len, head_dim]\n",
        "* K.transpose shape: [batch_size, num_heads, head_dim, seq_len]\n",
        "* When you multiply these, you get: [batch_size, num_heads, seq_len, seq_len]\n",
        "\n",
        "The key difference is that attention weights represent how much each token attends to every other token.\n",
        "\n",
        "Attention Weights:\n",
        "\n",
        "These tell you HOW MUCH each token should pay attention to every other token\n",
        "They are probabilities (sum to 1) showing the relative importance of each token relationship\n",
        "Shape: [batch_size, num_heads, seq_len, seq_len]\n",
        "Example: If token 1's attention weights are [0.7, 0.2, 0.1], it means it's paying 70% attention to token 1, 20% to token 2, and 10% to token 3\n",
        "\n",
        "Context Vectors:\n",
        "\n",
        "These are the actual NEW REPRESENTATIONS of each token after applying the attention\n",
        "They contain the weighted combination of information from all tokens based on the attention weights\n",
        "Shape: [batch_size, num_heads, seq_len, head_dim]\n",
        "Example: If V contains token representations [v1, v2, v3], and attention weights are [0.7, 0.2, 0.1], the context vector would be 0.7v1 + 0.2v2 + 0.1*v3\n",
        "\n",
        "In simpler terms:\n",
        "\n",
        "Attention weights tell you \"what to focus on\"\n",
        "Context vectors are \"what you learned\" after focusing on those things\n",
        "\n",
        "Think of it like reading a book:\n",
        "\n",
        "Attention weights are like highlighting parts of text (70% highlighted here, 20% there, etc.)\n",
        "Context vectors are the actual information you extracted after considering all those highlighted parts together\n",
        "\n",
        "\n",
        "Seq1Seq models use encoders and decoders. The encoder processes each item in the input sequence, it compiles the information it captures into a vector (called the context). After processing the entire input sequence, the encoder sends the context over to the decoder, which begins producing the output sequence item by item.\n",
        "\n",
        "Word embeddings turn words/tokens into a vector that capture a lot of the meaning/semantic information of the words.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WM_G4ANickS6"
      },
      "source": [
        "# notes from Dive Into Deep Learning Chapter on Attention & Transformers\n",
        "\n",
        "https://d2l.ai/chapter_attention-mechanisms-and-transformers/index.html\n",
        "\n",
        "In sequence to sequence models, some input sequences may be longer than others. For this reason, we must use token padding to artificially treat all inputs as the same length. To do this we can use a \"masked softmax\" where values beyond the valid lengths for each pair of vectors are all masked as zero.\n",
        "\n",
        "\n",
        "Multihead attention -> All heads operate on the same input in parallel but with different learned weights. This parallelism makes the model more expressive, as it’s learning multiple attention “patterns” at once.\n",
        "\n",
        "Had a question regarding how Q, K, and V matrices are actually learned. Answer from Claude:\n",
        "\n",
        "The key insight is that the Q, K, V matrices aren't directly supervised (we never tell them \"this is what a good query looks like\"). Instead, they learn useful transformations because:\n",
        "\n",
        "* If Q and K matrices learn to produce vectors that give high dot products for related tokens\n",
        "* And V matrices learn to produce useful value representations\n",
        "* Then the final output will better predict the next token\n",
        "* Which reduces the loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notes from the Illustrated Transformer Blog\n",
        "\n",
        "https://jalammar.github.io/illustrated-transformer/\n",
        "\n",
        "What is the difference between self attention and encoder-decoder attention?\n",
        "\n",
        "From Grok3:\n",
        "\n",
        "Self-Attention: Focuses on relationships within one sequence (input for the encoder, output for the decoder). It’s about internal context. In the decoder, it’s masked to enforce sequential generation.\n",
        "\n",
        "Encoder-Decoder Attention: Focuses on relationships between two sequences—the input (from the encoder) and the output (being generated by the decoder). It’s about alignment and translation relevance.\n",
        "\n",
        "### Attention:\n",
        "**As the model processes each word (each position in the input sequence), self attention allows it to look at other positions in the input sequence for clues that can help lead to a better encoding for this word.**\n",
        "* What are Query, Key, and Value vectors? They are just abstractions that are useful for calculating and thinking about attention.\n",
        "* Calculating an Attention Score is scoring each word of the input sequence against the current word the model is analyzing\n",
        "* The score determines how much emphasis to put on other parts of the input sentence as we encode a word at a certain position\n",
        "* Score is calculated by taking dot product of the **query vector** and **key vector** of the respective word we're scoring\n",
        "    * based on previous reading, during training the model learns to adjust the query and key matrices so that this operation returns a result that will tell it to attend to parts of the sentence that help it predict the correct answer!\n",
        "    * Given the input \"Thinking machines!\" -> the score for how much \"Thinking\" should attend to itself is q1 * k1, the score for how much it should attend to \"Machines\" is q1*k2\n",
        "    * From Grok: **The dot product measures similarity: a higher dot product means the query and key vectors are more aligned, indicating that the word associated with that key is more relevant to the word being processed.**\n",
        "    * **Through gradient descent, the model tweaks these matrices so that the dot products (q · k) yield higher scores for word pairs that are contextually relevant and lower scores for irrelevant ones.**\n",
        "\n",
        "* Next step is to divide the scores by the square root of the dimension of the key vectors to normalize them.\n",
        "* Then, we pass the results through a softmax function to ensure they are all positive and sum to 1.\n",
        "* After scaling (to avoid large values) and softmax normalization, these scores become the attention weights.\n",
        "* The softmax score determines how much each word will be expressed at this position\n",
        "* Next step is to multiple the values vectors by the softmax scores. Want to drown out values of irrelevant words by multiplying them by very small numbers (close to zero)\n",
        "* Next step is to sum up the values of the weighted vectors. This produces the output of the self-attention layer at this position (for the first word).\n",
        "* This resulting vector is the result of the self-attention mechanism and can then be passed on wards to the feed forward layer.\n",
        "\n",
        "### Matrix Calculation of Self-Attention\n",
        "\n",
        "1. Pack embeddings into a matrix. Multiply this matrix by the Q, K, V matrices to produce an output matrix of the results.\n",
        "2. Matrix multiply result of Q layer and transpose of K layer. Divide result by square root and pass through softmax function. Perform a matrix multiple of the result and the output from V layer.\n",
        "\n",
        "Wrote this as one step because this is considered one step in the blog post since we are dealing with matrices. Can easily do this in a single line of code if needed.\n",
        "\n",
        "### Multi-Head Attention\n",
        "\n",
        "With multi-headed attention, we maintain separate Q/K/V matrices for each head resulting in different Q/K/V matrices. Purpose is to expand the model's ability to focus on different positions. In the example above, the result from the attention mechanism contains a little information from other words (due to the Q * Key of that word), but it can be dominated by the word itself. **It gives the attention layer multiple “representation subspaces”.**\n",
        "\n",
        "The result from this is going to be a Z matrix for every head. The feed forward network is only expecting one matrix. We need a way to combine these matrices into one in a way that still contains all the information and context present when they are separate matrices. \n",
        "\n",
        "The blog suggests concatenating them and multiplying the matrix by another W matrix, that is trained as well. The result would be the Z matrix that captures information\n",
        "from all the attention heads. We can send this forward to the FFNN.\n",
        "\n",
        "### Positional Encodings\n",
        "\n",
        "We are missing positional encodings in this description. These allow the model take into account where in the input words occur. We use postional encoding vectors and combine them with the input embeddings to generate a new vector that has taken the positional encodings into account. This gives the model a sense of the order of the words.\n",
        "\n",
        "**The intuition here is that adding these values to the embeddings provides meaningful distances between the embedding vectors once they’re projected into Q/K/V vectors and during dot-product attention.**\n",
        "\n",
        "The blog does not go into detail on how positional encodings are derived. Might have to research that myself.\n",
        "\n",
        "### Decoder Side\n",
        "\n",
        "The encoder starts by processing input embeddings. There can be multiple encoders in a transformer architecture. The decoder takes the output of the last encoder as input, which is an attention weighted embedding vector for each word.\n",
        "\n",
        "The decoder then has to use its encoder-decoder attention mechanism to deconstruct this into K and V matrices. It does this by passing this input into its own K/V matrices with weights that have been trained. The decoder itself provides the Q values.\n",
        "\n",
        "1. Input to K and V: The encoder’s output (let’s call it H_enc) is passed into the encoder-decoder attention mechanism.\n",
        "2. Linear Projections: Inside this attention layer, H_enc is transformed into K and V using two separate learned weight matrices:\n",
        "    * K = H_enc * W_K (where W_K is the key projection matrix).\n",
        "    * V = H_enc * W_V (where W_V is the value projection matrix).\n",
        "3. Query from Decoder: The decoder generates its own Q vectors from its current internal state (the output of the masked self-attention layer), using a third weight matrix: Q = H_dec * W_Q.\n",
        "4. Attention Computation: The attention scores are computed as Q · K^T, normalized with softmax, and used to weight the V vectors, producing the final output of this layer.\n",
        "\n",
        "In the decoder, the self-attention layer is slightly different than the encoder. It is only allowed to attend to earlier positions in the output sentence here. This is done by by masking future positions (setting them to -inf) before the softmax step in the self-attention calculation.\n",
        "\n",
        "The output of the final decoder produces a vector of floats, but we need to translate this into a word. That’s the job of the final Linear layer which is followed by a Softmax Layer.\n",
        "\n",
        "The linear layer is a FFNN that converts the decoder vector into a much, much larger vector called a logits vector. The logits vector is the same length as the output vocabulary. For exmaple, if the output vocab is a 10,000 unique english words then the logits vector would be of length 10,000 - with each cell in the vector corresponding to a unique word in the vocab. The softmax layer then turns those scores into probabilities (all positive, all add up to 1.0). The cell with the highest probability is chosen, and the word associated with it is produced as the output for this time step.\n",
        "\n",
        "### Steps:\n",
        "1. Word Embeddings -> Convert input words into embeddings, or vectors, that represent numerically what the word is.\n",
        "2. Word embeddings for each word are passed through the each of the two layers of the Encoder (Self-Attention -> Feed Forward)\n",
        "    * each word flows through the exact same network individually\n",
        "    * each word embedding follows its own path through the encoder\n",
        "\n",
        "3. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
