{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "q-kvLAU4bk4f"
      },
      "outputs": [],
      "source": [
        "# Build an Attention Neural Network using PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionNeuralNet(nn.Module):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model    # embedding dimension (e.g., 512)\n",
        "    self.num_heads = num_heads  # number of attention heads (e.g., 8)\n",
        "    self.head_dim = d_model // num_heads  # dimension per head (e.g., 64)\n",
        "\n",
        "    # Create the Q, K, V projection layers\n",
        "    self.q_proj = nn.Linear(d_model, d_model)\n",
        "    self.k_proj = nn.Linear(d_model, d_model)\n",
        "    self.v_proj = nn.Linear(d_model, d_model)\n",
        "\n",
        "    # Final output projection\n",
        "    self.out_proj = nn.Linear(d_model, d_model)\n",
        "\n",
        "  # scaled dot product attention\n",
        "  def attention(self, Q, K, V):\n",
        "    \"\"\"\n",
        "    Q, K, V are expected to be of shape:\n",
        "      [batch_size, seq_len, d_k]\n",
        "    or possibly\n",
        "      [batch_size, num_heads, seq_len, d_k]\n",
        "    if youâ€™re already doing multi-head splitting.\n",
        "    \"\"\"\n",
        "    d_k = K.shape[-1]\n",
        "    scores = Q @ K.transpose(-2, -1)\n",
        "    scores = scores / math.sqrt(d_k)\n",
        "    attention_weights = F.softmax(scores, dim=-1)\n",
        "    output = attention_weights @ V\n",
        "\n",
        "    return output, attention_weights\n",
        "\n",
        "  def reshape_attention(self, output, attention_weights, batch_size, seq_len, d_model):\n",
        "    # re-order dimensions back to original\n",
        "    output = torch.permute(output, (0, 2, 1, 3))\n",
        "    # reshape the dimensions to \"combine\" the attention heads outputs\n",
        "    output = output.reshape(batch_size, seq_len, d_model)\n",
        "    # attention_weights has shape [batch_size, num_heads, seq_len, seq_len]\n",
        "    # Average across the heads dimension (dim=1)\n",
        "    attention_weights = attention_weights.mean(dim=1)\n",
        "\n",
        "    return output, attention_weights\n",
        "\n",
        "  def forward(self, x):\n",
        "    batch_size, seq_len, d_model = x.shape\n",
        "    Q = self.q_proj(x)\n",
        "    K = self.k_proj(x)\n",
        "    V = self.v_proj(x)\n",
        "\n",
        "    # Reshape to separate the heads\n",
        "    Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "    K = K.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "    V = V.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "\n",
        "    # re-order dimensions to be compatible with attention method\n",
        "    Q = torch.permute(Q, (0, 2, 1, 3))\n",
        "    K = torch.permute(K, (0, 2, 1, 3))\n",
        "    V = torch.permute(V, (0, 2, 1, 3))\n",
        "\n",
        "    output, attention_weights = self.attention(Q, K, V)\n",
        "    output, attention_weights = self.reshape_attention(output, attention_weights, batch_size, seq_len, d_model)\n",
        "\n",
        "    output = self.out_proj(output)\n",
        "\n",
        "    return output, attention_weights\n"
      ],
      "metadata": {
        "id": "xXKWE3ixbwqS"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_attention_shapes(batch_size=32, seq_len=10, d_model=512, num_heads=8):\n",
        "    # Create model\n",
        "    model = AttentionNeuralNet(d_model=d_model, num_heads=num_heads)\n",
        "\n",
        "    # Create dummy input\n",
        "    x = torch.randn(batch_size, seq_len, d_model)\n",
        "\n",
        "    # Forward pass\n",
        "    output, attention_weights = model(x)\n",
        "\n",
        "    # Check shapes\n",
        "    assert output.shape == (batch_size, seq_len, d_model)\n",
        "    assert attention_weights.shape == (batch_size, seq_len, seq_len)\n",
        "\n",
        "def test_attention_weights_sum_to_one(batch_size=32, seq_len=10, d_model=512, num_heads=8):\n",
        "  # Create model\n",
        "  model = AttentionNeuralNet(d_model=d_model, num_heads=num_heads)\n",
        "\n",
        "  # Create dummy input\n",
        "  x = torch.randn(batch_size, seq_len, d_model)\n",
        "\n",
        "  # Forward pass\n",
        "  output, attention_weights = model(x)\n",
        "  assert torch.allclose(attention_weights.sum(dim=-1), torch.ones_like(attention_weights.sum(dim=-1)))"
      ],
      "metadata": {
        "id": "nkFUUch9xZI8"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_attention_shapes()\n",
        "test_attention_weights_sum_to_one()"
      ],
      "metadata": {
        "id": "pKloLio3xnMm"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sequence_dataset(num_sequences=1000, seq_length=10, d_model=512):\n",
        "    # Create random input sequences\n",
        "    X = torch.randn(num_sequences, seq_length, d_model)\n",
        "    # Create target sequences (initially same as input)\n",
        "    y = X.clone()\n",
        "\n",
        "    # For positions 2, 5, 8, etc., make the target the sum of previous two tokens\n",
        "    for pos in range(2, seq_length, 3):\n",
        "        y[:, pos] = X[:, pos-1] + X[:, pos-2]\n",
        "\n",
        "    return X, y\n",
        "\n",
        "# Let's test the dataset creation\n",
        "def test_dataset():\n",
        "    X, y = create_sequence_dataset(num_sequences=5, seq_length=10, d_model=4)\n",
        "    print(\"Input shape:\", X.shape)\n",
        "    print(\"Target shape:\", y.shape)\n",
        "\n",
        "    # Verify the pattern for first sequence\n",
        "    print(\"\\nFirst sequence, first few dimensions:\")\n",
        "    print(\"Position 2 should equal sum of positions 0 and 1:\")\n",
        "    print(f\"X[0, 0]: {X[0, 0][:2]}\")  # First token\n",
        "    print(f\"X[0, 1]: {X[0, 1][:2]}\")  # Second token\n",
        "    print(f\"y[0, 2]: {y[0, 2][:2]}\")  # Third token (should be sum)\n",
        "\n",
        "# Training loop\n",
        "def train_attention_model(model, num_epochs=10):\n",
        "    X_train, y_train = create_sequence_dataset()\n",
        "    optimizer = torch.optim.Adam(model.parameters())\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        optimizer.zero_grad()\n",
        "        output, _ = model(X_train)\n",
        "        loss = criterion(output, y_train)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if epoch % 2 == 0:\n",
        "            print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")"
      ],
      "metadata": {
        "id": "pbffLYzk1Bn3"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=32\n",
        "seq_len=10\n",
        "d_model=512\n",
        "num_heads=8\n",
        "model = AttentionNeuralNet(d_model=d_model, num_heads=num_heads)\n",
        "train_attention_model(model, num_epochs=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3fUb-qg1HRJ",
        "outputId": "1073157c-07bb-4c7a-a9d3-9b9d5a0261a8"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 1.3141\n",
            "Epoch 2, Loss: 1.2966\n",
            "Epoch 4, Loss: 1.2799\n",
            "Epoch 6, Loss: 1.2622\n",
            "Epoch 8, Loss: 1.2419\n",
            "Epoch 10, Loss: 1.2172\n",
            "Epoch 12, Loss: 1.1863\n",
            "Epoch 14, Loss: 1.1490\n",
            "Epoch 16, Loss: 1.1069\n",
            "Epoch 18, Loss: 1.0641\n",
            "Epoch 20, Loss: 1.0253\n",
            "Epoch 22, Loss: 0.9915\n",
            "Epoch 24, Loss: 0.9599\n",
            "Epoch 26, Loss: 0.9270\n",
            "Epoch 28, Loss: 0.8917\n",
            "Epoch 30, Loss: 0.8549\n",
            "Epoch 32, Loss: 0.8181\n",
            "Epoch 34, Loss: 0.7823\n",
            "Epoch 36, Loss: 0.7476\n",
            "Epoch 38, Loss: 0.7134\n",
            "Epoch 40, Loss: 0.6789\n",
            "Epoch 42, Loss: 0.6442\n",
            "Epoch 44, Loss: 0.6099\n",
            "Epoch 46, Loss: 0.5767\n",
            "Epoch 48, Loss: 0.5448\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_model(model, seq_length=10, d_model=512):\n",
        "    # Create a test sequence\n",
        "    X_test = torch.randn(1, seq_length, d_model)\n",
        "    y_test = X_test.clone()\n",
        "    for pos in range(2, seq_length, 3):\n",
        "        y_test[:, pos] = X_test[:, pos-1] + X_test[:, pos-2]\n",
        "\n",
        "    # Get model predictions and attention weights\n",
        "    with torch.no_grad():\n",
        "        pred, attention_weights = model(X_test)\n",
        "\n",
        "    # Calculate prediction error\n",
        "    mse = nn.MSELoss()(pred, y_test)\n",
        "    print(f\"Test MSE: {mse.item():.4f}\")\n",
        "\n",
        "    # Analyze attention patterns\n",
        "    print(\"\\nAttention patterns for summed positions:\")\n",
        "    for pos in range(2, seq_length, 3):\n",
        "        print(f\"\\nPosition {pos} attention weights:\")\n",
        "        print(attention_weights[0, pos, pos-2:pos+1])  # Show attention to previous tokens\n",
        "\n",
        "def analyze_predictions(model, seq_length=10, d_model=512):\n",
        "    X_test = torch.randn(1, seq_length, d_model)\n",
        "    y_test = X_test.clone()\n",
        "\n",
        "    # Create expected sums\n",
        "    for pos in range(2, seq_length, 3):\n",
        "        y_test[:, pos] = X_test[:, pos-1] + X_test[:, pos-2]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        pred, _ = model(X_test)\n",
        "\n",
        "    # Compare predictions with expected sums\n",
        "    for pos in range(2, seq_length, 3):\n",
        "        expected_sum = X_test[0, pos-2] + X_test[0, pos-1]\n",
        "        print(f\"\\nPosition {pos}:\")\n",
        "        print(f\"Expected sum: {expected_sum[:5]}\")  # Show first 5 dimensions\n",
        "        print(f\"Prediction:   {pred[0, pos][:5]}\")"
      ],
      "metadata": {
        "id": "pWw69ys82iRU"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyze_model(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xm2H_tQS2j9P",
        "outputId": "99f490a2-56b3-417d-8d96-bb35e5bb6dbf"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test MSE: 0.9548\n",
            "\n",
            "Attention patterns for summed positions:\n",
            "\n",
            "Position 2 attention weights:\n",
            "tensor([0.0551, 0.0303, 0.6893])\n",
            "\n",
            "Position 5 attention weights:\n",
            "tensor([0.0646, 0.0256, 0.6734])\n",
            "\n",
            "Position 8 attention weights:\n",
            "tensor([0.0313, 0.0477, 0.5942])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyze_predictions(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xOerh2m3oQO",
        "outputId": "0b410320-8134-4047-fed0-10910dbf16b4"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Position 2:\n",
            "Expected sum: tensor([-3.8203, -0.2196, -0.1141, -0.7116,  0.2415])\n",
            "Prediction:   tensor([-0.2672, -0.2304,  0.3185,  0.7512,  1.3425])\n",
            "\n",
            "Position 5:\n",
            "Expected sum: tensor([ 0.1566, -0.0233,  0.6566, -1.4544, -2.7014])\n",
            "Prediction:   tensor([-1.0759, -1.0394,  0.6194, -1.0045,  0.4681])\n",
            "\n",
            "Position 8:\n",
            "Expected sum: tensor([ 0.1154, -0.6889,  1.8166, -0.6379, -1.6967])\n",
            "Prediction:   tensor([-0.3267, -0.0108, -0.0664, -0.3468, -0.2812])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NOTES\n",
        "\n",
        "Input embedding dimension is the embedding of each token's embedding matrix that comes into the attention layer. -> Split words into tokens and each token is converted to an embedding matrix that numerically represents what the word is in some language embedding space.\n",
        "\n",
        "Attention weights have a different dimension than the outputs.\n",
        "\n",
        "For the attention weights:\n",
        "\n",
        "They come from the scores calculation: scores = Q @ K.transpose(-2, -1)\n",
        "\n",
        "* Q shape: [batch_size, num_heads, seq_len, head_dim]\n",
        "* K.transpose shape: [batch_size, num_heads, head_dim, seq_len]\n",
        "* When you multiply these, you get: [batch_size, num_heads, seq_len, seq_len]\n",
        "\n",
        "The key difference is that attention weights represent how much each token attends to every other token."
      ],
      "metadata": {
        "id": "aWu-G96lid31"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xIGg6Muuiyoh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}